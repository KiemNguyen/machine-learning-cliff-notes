{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter's trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tab: Compelete  \n",
    "Shift + Tab Parameter: Quick inspect the parameters  \n",
    "Shift + Tab X2 or X3: More information  \n",
    "??function: Sourcode  \n",
    "?function: Documentation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorems, Lingos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Curse of dimensionality_: The more columns we have, the more empty space it creates. KNN works very well in high dimensions despite what the theory said because the points still have different distances away from each other.\n",
    "\n",
    "_No free lunch theorem_: In theory, there is no model that works well for any kind of dataset. In practice, we can use random forest for nearly all kind of dataset.\n",
    "\n",
    "_Churn_: Loss of users\n",
    "\n",
    "_Viral coefficient_: the ability of a business to tap into an early user base and get that user base to tell other people about it. If a site has a viral coefficient of 10%, and has 1,000 users, then after one month it will gain another 100 users through viral channels.\n",
    "\n",
    "_Ground truth_: Ground truth refers to strong labels that have a high likelihood of being accurate, knowledge that we're very sure is True.\n",
    "\n",
    "_Correlation does not imply causation!_: Just because we can see a connection or a mutual relationship between two variables, it doesn't mean that one causes the other.\n",
    "\n",
    "_A/B Testing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Regression_: Predict continous variables  \n",
    "_Classify_: Predict categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset contain a mix of continuous and categorical variables, convert everything into numbers  \n",
    "Always do feature extraction step when working with date-time, so we can capture any trend/cyclical behavior like holiday, weekend, sport event, rainning that day, etc. df.saledate.__dt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stuff after we process so we can accessed efficiently\n",
    "# os.makedirs('tmp', exist_ok=True)\n",
    "# df.to_feather('tmp/raw')\n",
    "\n",
    "# In the future we can simply read it from this last format:\n",
    "# df = pd.read_feather('tmp/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r^2 score: Ratio between how good my model is VS how good is the naive mean model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use _ variable if we want to throw something away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a decision tree from scratch:  \n",
    "    Calculate the root mean squared error of the 1st split. This number would represents how good a split is.\n",
    "    Try all variables and all possible value of that variable and see which variable and which value gives us a split with the best score.\n",
    "    Stop splitting when the leaf node only has 1 thing in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (Predict on uncorrelated trees):  \n",
    "    Grab some rows as random then put them into a smaller dataset and build a tree based on that\n",
    "    Do it again with a different random subset\n",
    "    Make prediction on each tree\n",
    "    Take an average  \n",
    "\n",
    "Use Out-of-bag (OOB) error to prevent over fitting or for small dataset: Pass un-used rows to the 1st tree and treat it as a validation set. Do the same thing for a 2nd tree. To calculation prediction, we would average all the trees where that row is not used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use Grid Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
