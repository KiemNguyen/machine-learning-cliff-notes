{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload reloads modules automatically before before executing user code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter's trick\n",
    "- Tab: Compelete  \n",
    "- Shift + Tab Parameter: Quick inspect the parameters  \n",
    "- Shift + Tab X2 or X3: More information  \n",
    "- ??function: Get soure code  \n",
    "- ?function: Get documentation\n",
    "- Running a bash command in a Jupyter notebook using !\n",
    "    - !ls {PATH} Python variables must be inside the {}\n",
    "    - !ls -lh: Get size of a file\n",
    "    - !wc -l file_name: Get number of rows of a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "- Pandas works really well with numpy: We can apply a numpy function on a Pandas Series. Ex: df.SalePrice = np.log(df.SalePrice)\n",
    "- Remove a column from DataFrame: df.drop(column_name, axis = 1)\n",
    "- Useful read_csv parameters\n",
    "```python\n",
    "pd.read_csv(f'{PATH}Train.csv', low_memory=False, parse_dates=[\"saledate\"])\n",
    "# low_memory=False : parse all dtypes of the file\n",
    "# parse_dates=[] : give all columns that are with dtype data (and will convert them to DataTime dtype)\n",
    "```\n",
    "- A good practice is to save/load DataFrame by using feather so we can access it efficiently:\n",
    "```python\n",
    "os.makedirs('tmp', exist_ok=True)  # Create a folder\n",
    "df.to_feather('tmp/raw')  # Save\n",
    "df = pd.read_feather('tmp/raw')  # Load\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set\n",
    "- If the dataset contain a mix of continuous and categorical variables, convert everything into numbers.\n",
    "- Always do feature extraction step when working with __date-time__, so we can capture any trend/cyclical behavior like holiday, weekend, sport event, rainning that day, etc. df.saledate.__dt.__\n",
    "- Always do as much of our works as we can on a small sample of the data.\n",
    "- Remove outliers which make sense and there is no other variable to capture those outlier. For example: If the store has extra sale data before and after closing period, and we donâ€™t have any data to model the outliers then we should remove them during training.\n",
    "- If we don't have a good validation set, it's impossible to create a good model.\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate success and iterate\n",
    "- Cross validation and testing\n",
    "- Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype a Machine Learning Algorithm\n",
    "_Regression_: Predict continous variables  \n",
    "_Classify_: Predict categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorems, Lingos, Terms\n",
    "_Curse of dimensionality_: The more columns we have, the more empty space it creates. KNN works very well in high dimensions despite what the theory said because the points still have different distances away from each other.\n",
    "\n",
    "_No free lunch theorem_: In theory, there is no model that works well for any kind of dataset. In practice, we can use random forest for nearly all kind of dataset.\n",
    "\n",
    "_Churn_: Loss of users\n",
    "\n",
    "_Viral coefficient_: the ability of a business to tap into an early user base and get that user base to tell other people about it. If a site has a viral coefficient of 10%, and has 1,000 users, then after one month it will gain another 100 users through viral channels.\n",
    "\n",
    "_Ground truth_: Ground truth refers to strong labels that have a high likelihood of being accurate, knowledge that we're very sure is True.\n",
    "\n",
    "_Correlation does not imply causation!_: Just because we can see a connection or a mutual relationship between two variables, it doesn't mean that one causes the other.\n",
    "\n",
    "_Cardinality_: The number of levels in a category (Sex has a cardinality of 2, zipcode has a cardinality of 5000)\n",
    "\n",
    "_A/B Testing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tips\n",
    "- r^2 score: Ratio between how good my model is VS how good is the naive mean model\n",
    "- In python use _ variable if we want to throw something away\n",
    "- Tips when doing projects:\n",
    "    - Build a Random Forest as fast as we can, try to get into the point that it's significan better than random.\n",
    "    - Plot feature important.\n",
    "    - Talk to client about the important features, what does that mean, where does it come from. They might disagree with important feature from our plots. Thus client might miss-understanding of the data they gave us and we have a __data leakage__ problem (There is information in the data set that we were modeling with which client wouldn't have had in real life at the point when they were making a decision) \n",
    "    - Start throwing out unimportant features away.\n",
    "    - Create a new Random Forest to see if our score doesn't change much.\n",
    "    - Plot feature important again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forest constructs a multiple of decision trees at training time and output the class labels (Classification) or mean prediction (Regression) of the individual trees.\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=20, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "Random Forest is kind of universal ML algo works with any kind of data. It's not overfit, doesn't assume our data is normally distributed, doesn't assume the relationship is linear, require a little of feature engineering. \n",
    "- Build a decision tree from scratch:  \n",
    "    - Calculate the root mean squared error of the 1st split. This number would represents how good a split is.\n",
    "    - Try all variables and all possible value of that variable and see which variable and which value gives us a split with the best score.\n",
    "    - Stop splitting when the leaf node only has 1 thing in it.\n",
    "- Random Forest (Predict on uncorrelated trees):  \n",
    "    - Grab some rows as random then put them into a smaller dataset and build a tree based on that\n",
    "    - Do it again with a different random subset\n",
    "    - Make prediction on each tree\n",
    "    - Take an average  \n",
    "- Use Out-of-bag (OOB) error to prevent over fitting or for small dataset: Pass un-used rows to the 1st tree and treat it as a validation set. Do the same thing for a 2nd tree. To calculation prediction, we would average all the trees where that row is not used for training\n",
    "- Use standard deviation of prediction, instead of just the mean, to find out confidence of our estimate after the model averages predictions across tree. This would tell us the relative confidence of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce over-overfitting\n",
    "- Subsampling: The easiest way to avoid over-fitting is also the best way to speed up analysis. Rather then limit the total amount of data our model can access, we just limit it to a _different_ random subset per tree. Given enough trees, the model can still see _all_ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing redundant features\n",
    "Variables with very similar meanings will make it harder to interpret. We can try to remove redundant features by using a dendrogram (hierarchical clustering).\n",
    "### Partial dependence\n",
    "A powerful technique to find out for the features that are important, how do they realate to the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
