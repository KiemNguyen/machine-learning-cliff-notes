{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload reloads modules automatically before before executing user code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter's trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Tab: Compelete  \n",
    "- Shift + Tab Parameter: Quick inspect the parameters  \n",
    "- Shift + Tab X2 or X3: More information  \n",
    "- ??function: Get soure code  \n",
    "- ?function: Get documentation\n",
    "- Running a bash command in a Jupyter notebook using !\n",
    "    - !ls {PATH} Python variables must be inside the {}\n",
    "    - !ls -lh: Get size of a file\n",
    "    - !wc -l file_name: Get number of rows of a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas works really well with numpy: You can apply a numpy function on a Pandas Series. Ex: df.SalePrice = np.log(df.SalePrice)\n",
    "- Remove a column from DataFrame: df.drop(column_name, axis = 1)\n",
    "- pd.read_csv(f'{PATH}Train.csv', low_memory = False, parse_dates = [\"saledate\"])\n",
    "    - low_memory = False : parse all dtypes of the file\n",
    "    - parse_dates = [] : give all columns names that are with dtype data (and will convert them to DataTime dtype)\n",
    "- A good practice is to save/load DataFrame by using feather so we can access it efficiently:\n",
    "    - Create a folder: os.makedirs('tmp', exist_ok=True)\n",
    "    - Save: df.to_feather('tmp/raw')\n",
    "    - Load: df = pd.read_feather('tmp/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the dataset contain a mix of continuous and categorical variables, convert everything into numbers.\n",
    "- Always do feature extraction step when working with __date-time__, so we can capture any trend/cyclical behavior like holiday, weekend, sport event, rainning that day, etc. df.saledate.__dt.__\n",
    "- Always do as much of your works as you can on a small sample of the data.\n",
    "- Remove outliers which make sense and there is no other variable to capture those outlier. For example: If the store has extra sale data before and after closing period, and you donâ€™t have any data to model the outliers then you should remove them during training.\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate success and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation and testing\n",
    "- Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype a Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Regression_: Predict continous variables  \n",
    "_Classify_: Predict categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorems, Lingos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Curse of dimensionality_: The more columns we have, the more empty space it creates. KNN works very well in high dimensions despite what the theory said because the points still have different distances away from each other.\n",
    "\n",
    "_No free lunch theorem_: In theory, there is no model that works well for any kind of dataset. In practice, we can use random forest for nearly all kind of dataset.\n",
    "\n",
    "_Churn_: Loss of users\n",
    "\n",
    "_Viral coefficient_: the ability of a business to tap into an early user base and get that user base to tell other people about it. If a site has a viral coefficient of 10%, and has 1,000 users, then after one month it will gain another 100 users through viral channels.\n",
    "\n",
    "_Ground truth_: Ground truth refers to strong labels that have a high likelihood of being accurate, knowledge that we're very sure is True.\n",
    "\n",
    "_Correlation does not imply causation!_: Just because we can see a connection or a mutual relationship between two variables, it doesn't mean that one causes the other.\n",
    "\n",
    "_A/B Testing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  r^2 score: Ratio between how good my model is VS how good is the naive mean model\n",
    "-  In python use _ variable if we want to throw something away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "model = RandomForestRegressor(n_jobs = -1)\n",
    "model.fit(df.drop('SalePrice', axis = 1), df.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is kind of universal ML algo works with any kind of data. It's not overfit, doesn't assume your data is normally distributed, doesn't assume the relationship is linear, require a little of feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a decision tree from scratch:  \n",
    "    Calculate the root mean squared error of the 1st split. This number would represents how good a split is.\n",
    "    Try all variables and all possible value of that variable and see which variable and which value gives us a split with the best score.\n",
    "    Stop splitting when the leaf node only has 1 thing in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (Predict on uncorrelated trees):  \n",
    "    Grab some rows as random then put them into a smaller dataset and build a tree based on that\n",
    "    Do it again with a different random subset\n",
    "    Make prediction on each tree\n",
    "    Take an average  \n",
    "\n",
    "Use Out-of-bag (OOB) error to prevent over fitting or for small dataset: Pass un-used rows to the 1st tree and treat it as a validation set. Do the same thing for a 2nd tree. To calculation prediction, we would average all the trees where that row is not used for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
